import os
import hashlib
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, PayloadSchemaType

from util.path import PDF_DIR

load_dotenv()

class PdfVectorizer:
    """ì§€ì •ëœ í´ë” ë‚´ ëª¨ë“  pdf íŒŒì¼ì„ ì„ë² ë”© í›„ Qdrantì— ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(
        self,
        folder_path: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 100,
        embedding_model: str = "text-embedding-3-small",
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "samsung_external_pdf"
    ):
        self.folder_path = folder_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.embedding = OpenAIEmbeddings(model=embedding_model)
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.qdrant_client = QdrantClient(url=qdrant_url)
        
        # ğŸ”‘ ì´ˆê¸°í™” ì‹œ ì¸ë±ìŠ¤ ì„¤ì •
        self._ensure_index()

    def _ensure_index(self):
        """file_hash í•„ë“œì— ì¸ë±ìŠ¤ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ìŠ¤í‚µ)"""
        try:
            # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸(collection ì—†ëŠ”ë° index ìƒì„± ì˜¤ë¥˜ ë°©ì§€)
            collections = self.qdrant_client.get_collections()
        
            if not any(c.name == self.collection_name for c in collections.collections):
                print(f"Collection '{self.collection_name}' ì—†ìŒ - ì²« ì‹¤í–‰ ì €ì¥ ì´í›„ _ensure_index ë‹¤ì‹œ í˜¸ì¶œ ë¨")
                return
            
            # ì¸ë±ìŠ¤ ìƒì„±(ê¸°ì¡´ì— ê°™ì€ indexê°€ ìˆìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°€ë¯€ë¡œ, Exceptionì€ ë°œìƒí•˜ì§€ ì•ŠìŒ)
            self.qdrant_client.create_payload_index(
                collection_name=self.collection_name,
                field_name="metadata.file_hash",
                field_schema=PayloadSchemaType.KEYWORD
            )
        
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ìƒì„± ì—ëŸ¬: {e}")

    def calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ì˜ SHA256 í•´ì‹œ ê³„ì‚° (ì²­í‚¹ ì „ì— ë¹ ë¥´ê²Œ ê³„ì‚°)"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def is_already_processed(self, file_hash: str) -> bool:
        """Qdrantì— ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸"""
        try:
            collections = self.qdrant_client.get_collections()
            collection_exists = any(
                c.name == self.collection_name 
                for c in collections.collections
            )
            
            if not collection_exists:
                return False
            
            result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter=Filter(
                    must=[
                        FieldCondition(
                            key="metadata.file_hash",
                            match=MatchValue(value=file_hash)
                        )
                    ]
                ),
                limit=1,
                with_payload=False,
                with_vectors=False
            )
            
            return len(result[0]) > 0
            
        except Exception as e:
            print(f"  [DEBUG] ì—ëŸ¬ ë°œìƒ: {e}")
            return False

    def find_pdf_files(self):
        """í´ë” ë‚´ì˜ ëª¨ë“  pdf íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘"""
        if not os.path.exists(self.folder_path):
            raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.folder_path}")

        pdf_files = [
            os.path.join(self.folder_path, f)
            for f in os.listdir(self.folder_path)
            if f.lower().endswith(".pdf")
        ]

        if not pdf_files:
            print(f"í´ë” ë‚´ì— pdf íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.folder_path}")
        else:
            print(f"Found {len(pdf_files)} PDF files in folder: {self.folder_path}")

        return pdf_files

    def load_pdfs(self, pdf_files_with_hash):
        """PDF íŒŒì¼ë“¤ì„ ë¡œë“œ (í•´ì‹œê°’ê³¼ í•¨ê»˜)"""
        all_docs = []
        for path, file_hash in pdf_files_with_hash:
            try:
                loader = PDFPlumberLoader(path)
                docs = loader.load()
                
                # ê° ë¬¸ì„œì— file_hash ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in docs:
                    doc.metadata["file_hash"] = file_hash
                    doc.metadata["file_name"] = os.path.basename(path)
                    doc.metadata["processed_at"] = datetime.now().isoformat()

                print(f"Loaded {len(docs)} pages from {os.path.basename(path)}")
                all_docs.extend(docs)
            except Exception as e:
                print(f"{os.path.basename(path)} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ì´ {len(all_docs)} pages ë¡œë“œ ì™„ë£Œ (íŒŒì¼ {len(pdf_files_with_hash)}ê°œ).")
        return all_docs

    def split_documents(self, docs):
        """ë¬¸ì„œë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í• """
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )

        split_docs = splitter.split_documents(docs)
        print(f"Split into {len(split_docs)} chunks.")

        return split_docs

    def store_in_qdrant(self, docs):
        """Qdrantì— ë¬¸ì„œ ì €ì¥"""
        print(f"Saving {len(docs)} chunks to Qdrant collection: {self.collection_name}")

        Qdrant.from_documents(
            documents=docs,
            embedding=self.embedding,
            collection_name=self.collection_name,
            url=self.qdrant_url
        )

        print(f"Successfully stored {len(docs)} docs in '{self.collection_name}'.\n")
        
        # ğŸ”‘ ì €ì¥ í›„ ì¸ë±ìŠ¤ ë‹¤ì‹œ í™•ì¸ (ì²« ì‹¤í–‰ ì‹œ ì»¬ë ‰ì…˜ì´ ë°©ê¸ˆ ìƒì„±ë¨)
        self._ensure_index()

    def run(self):
        """í´ë” ë‚´ PDF íŒŒì¼ ì „ì²´ë¥¼ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° í¬í•¨)"""
        pdf_files = self.find_pdf_files()

        if not pdf_files:
            return

        # ì¤‘ë³µ ì²´í¬: ìƒˆ íŒŒì¼ë§Œ í•„í„°ë§
        new_files = []
        skipped_count = 0
        
        print("\n=== ì¤‘ë³µ ì²´í¬ ì‹œì‘ ===")
        for pdf_path in pdf_files:
            file_hash = self.calculate_file_hash(pdf_path)
            
            if self.is_already_processed(file_hash):
                print(f"â­ï¸  Skip: {os.path.basename(pdf_path)} (ì´ë¯¸ ì²˜ë¦¬ë¨)")
                skipped_count += 1
            else:
                print(f"âœ… New: {os.path.basename(pdf_path)}")
                new_files.append((pdf_path, file_hash))
        
        print(f"\nì´ {len(pdf_files)}ê°œ íŒŒì¼ ì¤‘:")
        print(f"  - ìƒˆ íŒŒì¼: {len(new_files)}ê°œ")
        print(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ")
        print("=" * 40 + "\n")

        if not new_files:
            print("âœ… ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ìƒˆ íŒŒì¼ë§Œ ë¡œë“œ/ì²­í‚¹/ì €ì¥
        docs = self.load_pdfs(new_files)

        if not docs:
            print("ë¡œë“œëœ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        split_docs = self.split_documents(docs)
        self.store_in_qdrant(split_docs)


if __name__ == "__main__":
    pdf_vectorizer = PdfVectorizer(
        folder_path=PDF_DIR,
        chunk_size=1000,
        chunk_overlap=100,
        qdrant_url="http://localhost:6333",
        collection_name="samsung_external_pdf",
        embedding_model="text-embedding-3-small"
    )
    pdf_vectorizer.run()