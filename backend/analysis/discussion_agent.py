"""
Discussion Agent - LangGraph ê¸°ë°˜ ì»¨ì„¤í„´íŠ¸ ì¸í„°ë·° ë° ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ ì „ë¬¸ê°€(ì»¨ì„¤í„´íŠ¸) í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•˜ì—¬ ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ ì¸í„°ë·°ë¥¼ ìˆ˜í–‰í•˜ê³ ,
ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""
import operator
from typing import Annotated, List, Callable

from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage, get_buffer_string
from langchain_core.runnables import RunnableConfig
from langchain_core.prompts import load_prompt
from langchain_tavily import TavilySearch
from langchain_community.vectorstores import Qdrant
from langchain_community.utilities import SQLDatabase

from langgraph.graph import START, END, StateGraph, MessagesState
from langgraph.graph.state import CompiledStateGraph
from langgraph.checkpoint.memory import MemorySaver

from qdrant_client import QdrantClient
from sqlalchemy import text


# ================================
# 1. í™˜ê²½ ì„¤ì •
# ================================

load_dotenv()

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model='gpt-4o', temperature=0)


# ================================
# 2. ë°ì´í„° ëª¨ë¸ ì •ì˜
# ================================

class Consultant(BaseModel):
    """ì»¨ì„¤í„´íŠ¸ í˜ë¥´ì†Œë‚˜ ëª¨ë¸"""
    department: str
    name: str
    role: str
    description: str
    
    @property
    def persona(self) -> str:
        return f"Department: {self.department}\nRole: {self.role}\nName: {self.name}\nDescription: {self.description}\n"


class SearchQuery(BaseModel):
    """ê²€ìƒ‰ ì¿¼ë¦¬ ëª¨ë¸"""
    search_query: str = Field(None, description="Search query for retrieval.")


class Perspectives(BaseModel):
    """ì„¹ì…˜ë³„ ê´€ì  ëª¨ë¸"""
    sections: List[str] = Field(
        description="List of comprehensive perspectives on the topic.",
    )


# ================================
# 3. ìƒíƒœ ì •ì˜
# ================================

def debug_reducer(old, new):
    """State update ë™ê¸°í™” error í•´ê²°ìš© reducer"""
    return new


class InterviewState(MessagesState):
    """ì¸í„°ë·° ìƒíƒœ"""
    max_num_turns: int
    context: Annotated[list, operator.add]
    consultant: Consultant
    interview: str
    sections: list
    topic: Annotated[str, debug_reducer]


class ResearchGraphState(MessagesState):
    """ì „ì²´ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ìƒíƒœ"""
    topic: Annotated[str, debug_reducer]
    max_consultants: int
    human_consultant_feedback: str
    consultants: List[Consultant]
    sections: Annotated[list, operator.add]
    introduction: str
    content: str
    conclusion: str
    final_report: str


# ================================
# 4. ì»¨ì„¤í„´íŠ¸ ì„¤ì •
# ================================

# ì‚¬ì „ ì •ì˜ëœ ì»¨ì„¤í„´íŠ¸ë“¤
chief_consultant = Consultant(
    department="AI",
    name="Peter Thiel",
    role="AI ìˆ˜ì„ ì»¨ì„¤í„´íŠ¸",
    description="""
        í˜ì´íŒ” ê³µë™ì°½ì—…ìì´ì ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì˜ ì‚¬ìƒì  ì „ëµê°€ë¡œ, 'ì œë¡œ íˆ¬ ì›' ì² í•™ì„ ì£¼ì°½í•œ í˜ì‹ ê°€ì…ë‹ˆë‹¤.  
        ë‹¨ìˆœí•œ ê²½ìŸë³´ë‹¤ ë…ì°½ì  ì‹œì¥ ì°½ì¶œì„ ì¤‘ì‹œí•˜ë©°, ë¹„ì „ì„ êµ¬ì²´ì  ì‹¤í–‰ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ëŠ¥ë ¥ì´ íƒì›”í•©ë‹ˆë‹¤.  
        ê¸°ì¡´ ì§ˆì„œë¥¼ ì˜ì‹¬í•˜ê³  ë¯¸ë˜ë¥¼ ê±°ì‹œì ìœ¼ë¡œ ì„¤ê³„í•˜ëŠ” ëƒ‰ì² í•œ ì‚¬ìƒ‰ê°€ì´ì íˆ¬ììì…ë‹ˆë‹¤.
    """,
)

financial_consultant = Consultant(
    department="AI",
    name="Warren Buffett",
    role="ê¸ˆìœµ ì „ë¬¸ê°€",
    description="""
        íˆ¬ì ì² í•™ì˜ ëŒ€ê°€ë¡œ, ì¥ê¸°ì  ê´€ì ì—ì„œ ê¸°ì—…ì˜ ë³¸ì§ˆ ê°€ì¹˜ì— ì§‘ì¤‘í•˜ëŠ” ë³´ìˆ˜ì  íˆ¬ììì…ë‹ˆë‹¤.  
        í•©ë¦¬ì  ì‚¬ê³ ì™€ ì¸ë‚´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ„ê¸° ì†ì—ì„œë„ ëƒ‰ì •í•˜ê²Œ íŒë‹¨í•˜ë©°, ë³µë¦¬ì˜ í˜ì„ ì‹ ë¢°í•©ë‹ˆë‹¤.  
        ë‹¨ê¸°ì  ìœ í–‰ë³´ë‹¤ ê²½ì˜ì§„ì˜ ì‹ ë¢°ë„ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ì§€ì†ê°€ëŠ¥ì„±ì„ ì¤‘ì‹œí•˜ëŠ” ì‹¤ìš©ì  ë¦¬ë”ì…ë‹ˆë‹¤.
    """,
)

hardware_consultant = Consultant(
    department="AI",
    name="Jensen Huang",
    role="í•˜ë“œì›¨ì–´ ì „ë¬¸ê°€",
    description="""
        ì—”ë¹„ë””ì•„ì˜ ê³µë™ ì°½ë¦½ìì´ì ê¸°ìˆ  ë¹„ì „ì„ í˜„ì‹¤ë¡œ ì´ë„ëŠ” í˜ì‹ ì  ë¦¬ë”ì…ë‹ˆë‹¤.  
        AIì™€ GPU ì»´í“¨íŒ… ì‹œëŒ€ë¥¼ ì„ ë„í•˜ë©°, ë³µì¡í•œ ê¸°ìˆ ì„ ì‹œì¥ ì¤‘ì‹¬ì˜ ì „ëµìœ¼ë¡œ ì „í™˜í•˜ëŠ” ëŠ¥ë ¥ì´ íƒì›”í•©ë‹ˆë‹¤.  
        ì—´ì •ì ì´ê³  ì¹´ë¦¬ìŠ¤ë§ˆ ìˆëŠ” ë¦¬ë”ì‹­ìœ¼ë¡œ íŒ€ì˜ ëª°ì…ê³¼ ì°½ì˜ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
    """,
)

software_consultant = Consultant(
    department="AI",
    name="Mark Zuckerberg",
    role="ì†Œí”„íŠ¸ì›¨ì–´ ì „ë¬¸ê°€",
    description="""
        ì†Œì…œ ë„¤íŠ¸ì›Œí¬ í˜ì‹ ì„ ì£¼ë„í•œ ê¸°ì—…ê°€ë¡œ, ë°ì´í„° ì¤‘ì‹¬ ì‚¬ê³ ì™€ ì‹¤í—˜ì  ì ‘ê·¼ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤.  
        ë¹ ë¥¸ ì˜ì‚¬ê²°ì •ê³¼ ë°˜ë³µì  ê°œì„ ì„ í†µí•´ ëŒ€ê·œëª¨ í”Œë«í¼ì„ ì„±ì¥ì‹œí‚¨ ì‹¤í–‰ ì¤‘ì‹¬í˜• ë¦¬ë”ì…ë‹ˆë‹¤.  
        ìµœê·¼ì—ëŠ” ë©”íƒ€ë²„ìŠ¤ì™€ AIë¥¼ ê²°í•©í•œ ë¯¸ë˜ ì—°ê²° ìƒíƒœê³„ êµ¬ì¶•ì— ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    """,
)

# ê¸°ë³¸ ì»¨ì„¤í„´íŠ¸ ë¦¬ìŠ¤íŠ¸
consultants = [financial_consultant, hardware_consultant, software_consultant]


# ================================
# 5. ì¸í„°ë·° ë…¸ë“œ í•¨ìˆ˜ë“¤
# ================================
def ask_question(state: InterviewState):
    """ì»¨ì„¤í„´íŠ¸ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    consultant = state['consultant']
    messages = state['messages']
    
    question_instructions = load_prompt("../prompt/question_instructions.yaml", encoding="utf-8")
    
    system_message = question_instructions.format(goals=consultant.persona)
    question = llm.invoke([SystemMessage(content=system_message)] + messages)
    
    return {'messages': [question]}

def search_web(state: InterviewState):
    """ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë…¸ë“œ"""
    search_instructions = load_prompt("../prompt/search_instructions.yaml", encoding="utf-8").format()
    
    # ì‘ë‹µ í˜•ì‹ ì§€ì •
    structured_llm = llm.with_structured_output(SearchQuery)
    # LLMìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
    search_query = structured_llm.invoke([SystemMessage(search_instructions)] + state['messages'])
    
    tavily_search = TavilySearch(max_results=3)
    search_docs = tavily_search.invoke(search_query.search_query)
    
    return {"context": [search_docs]}


def create_retriever(collection_name: str, k: int = 3):
    """Qdrantì—ì„œ retriever ìƒì„±"""
    QDRANT_URL = 'http://localhost:6333'
    
    client = QdrantClient(url=QDRANT_URL)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    vectorstore = Qdrant(
        client=client,
        collection_name=collection_name,
        embeddings=embeddings
    )
    
    return vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": k}
    )


def search_rag(state: InterviewState):
    """RAGë¥¼ í†µí•´ ë‚´ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ"""
    COLLECTIONS = ['samsung_internal_db', 'samsung_external_web', 'samsung_external_pdf']
    retrievers = {name: create_retriever(name) for name in COLLECTIONS}
    
    query = state['topic']
    
    for name, retriever in retrievers.items():
        results = retriever.invoke(query)
        print(f"\nğŸ“‚ {name} results:")
        for doc in results[:2]:
            print("-", doc.metadata.get("source"), ":", doc.page_content[:100])
            
        return {"context": [results]}

def answer_question(state: InterviewState):
    """ì „ë¬¸ê°€ê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    consultant = state['consultant']
    messages = state['messages']
    context = state.get('context', [])
    
    answer_instructions = load_prompt("../prompt/answer_instructions.yaml", encoding="utf-8")
    
    system_message = answer_instructions.format(
        goals=consultant.persona,
        context=context
    )
    answer = llm.invoke([SystemMessage(content=system_message)] + messages)
    
    return {'messages': [answer]}


def save_interview(state: InterviewState):
    """ì¸í„°ë·° ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” ë…¸ë“œ"""
    messages = state['messages']
    interview = get_buffer_string(messages)
    
    return {'interview': interview}

def write_section(state: InterviewState):
    """ì¸í„°ë·° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¹ì…˜ì„ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    interview = state['interview']
    context = state['context']
    consultant = state['consultant']
    
    # TODO interview ë‚´ìš© í™•ì¸í•˜ì—¬ í™œìš© ê²°ì •
    print(interview)
    
    section_writer_instructions = load_prompt("../prompt/section_writer_instructions.yaml", encoding="utf-8")
    
    system_message = section_writer_instructions.format(focus=consultant.persona)
    section = llm.invoke([
        SystemMessage(content=system_message),
        HumanMessage(content=f"Use this source material to create a section: {context}")
    ])
    
    return {'sections': [section.content]}


def route_messages(state: InterviewState):
    """ëŒ€í™” ì¢…ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜"""
    messages = state['messages']
    max_num_turns = state.get('max_num_turns', 2)
    num_responses = len([m for m in messages if isinstance(m, AIMessage)])
    
    if num_responses >= max_num_turns:
        return 'save_interview'
    
    last_question = messages[-2]
    
    if "Thank you so much for your help" in last_question.content:
        return 'save_interview'
    
    return 'ask_question'


# ================================
# 6. ì¸í„°ë·° ê·¸ë˜í”„ êµ¬ì„±
# ================================

def build_interview_graph():
    """ì¸í„°ë·° ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê³  ë°˜í™˜"""
    interview_builder = StateGraph(InterviewState)
    
    # ë…¸ë“œ ì¶”ê°€
    interview_builder.add_node('ask_question', ask_question)
    interview_builder.add_node('search_web', search_web)
    interview_builder.add_node('search_rag', search_rag)
    interview_builder.add_node('answer_question', answer_question)
    interview_builder.add_node('save_interview', save_interview)
    interview_builder.add_node('write_section', write_section)
    
    # ì—£ì§€ ì—°ê²°
    interview_builder.add_edge(START, 'ask_question')
    interview_builder.add_edge('ask_question', 'search_web')
    interview_builder.add_edge('ask_question', 'search_rag')
    interview_builder.add_edge('search_web', 'answer_question')
    interview_builder.add_edge('search_rag', 'answer_question')
    interview_builder.add_conditional_edges(
        'answer_question', route_messages, ['ask_question', 'save_interview']
    )
    interview_builder.add_edge('save_interview', 'write_section')
    interview_builder.add_edge('write_section', END)
    
    memory = MemorySaver()
    return interview_builder.compile(checkpointer=memory).with_config(
        run_name='Conduct Interviews'
    )


# ================================
# 7. ë©”ì¸ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ë“¤
# ================================

def setting_consultants(state: ResearchGraphState):
    """ì»¨ì„¤í„´íŠ¸ë“¤ì„ ì„¤ì •í•˜ëŠ” ë…¸ë“œ"""
    
    return {'consultants': consultants}

# TODO ë³‘ë ¬ì‹¤í–‰ì„ ìœ„í•´ ì¶”ê°€í•œ ë”ë¯¸ í•¨ìˆ˜ - ë¶ˆí•„ìš”ì‹œ ì œê±°
# ë…¸ë“œ í•¨ìˆ˜ (ìƒíƒœ ì—…ë°ì´íŠ¸ë§Œ)
def initiate_all_interviews_node(state: ResearchGraphState):
    # ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜ (pass-through) 
    return {}

def route_to_interviews(state: ResearchGraphState):
    """ì¸í„°ë·° ë¼ìš°íŒ… í•¨ìˆ˜"""
    max_consultants = state.get('max_consultants', 3)
    
    return [
        Send("conduct_interview", {
            "consultant": consultant,
            "messages": [
                HumanMessage(content="Welcome! We'll begin the interview now.")
            ],
            "max_num_turns": 2,
            "context": [],
            "topic": state['topic'],
            "sections": state['sections']
        })
        for consultant in state['consultants'][:max_consultants]
    ]

def write_report(state: ResearchGraphState):
    """ë©”ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    sections = state['sections']
    topic = state['topic']
    
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])
    
    report_writer_instructions = load_prompt("../prompt/report_writer_instructions.yaml", encoding="utf-8")
    
    system_message = report_writer_instructions.format(
        topic=topic,
        context=formatted_str_sections
    )
    report = llm.invoke([SystemMessage(content=system_message)])
    
    return {'content': report.content}

def write_introduction(state: ResearchGraphState):
    """ì¸íŠ¸ë¡œë¥¼ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    topic = state['topic']
    sections = state['sections']
    
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])
    
    intro_conclusion_instructions = load_prompt("../prompt/intro_conclusion_instructions.yaml", encoding="utf-8")
    
    instructions = intro_conclusion_instructions.format(
        topic=topic,
        formatted_str_sections=formatted_str_sections
    )
    intro = llm.invoke([
        SystemMessage(content=instructions),
        HumanMessage(content=f"Here is the main body:\n\n{formatted_str_sections}")
    ])
    
    return {'introduction': intro.content}


def write_conclusion(state: ResearchGraphState):
    """ê²°ë¡ ì„ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    topic = state['topic']
    sections = state['sections']
    
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])
    
    intro_conclusion_instructions = load_prompt("../prompt/intro_conclusion_instructions.yaml", encoding="utf-8")
    
    instructions = intro_conclusion_instructions.format(
        topic=topic,
        formatted_str_sections=formatted_str_sections
    )
    conclusion = llm.invoke([
        SystemMessage(content=instructions),
        HumanMessage(content=f"Here is the main body:\n\n{formatted_str_sections}")
    ])
    
    return {'conclusion': conclusion.content}


def finalize_report(state: ResearchGraphState):
    """ìµœì¢… ë³´ê³ ì„œë¥¼ ì™„ì„±í•˜ëŠ” ë…¸ë“œ"""
    introduction = state['introduction']
    content = state['content']
    conclusion = state['conclusion']
    topic = state['topic']
    
    final_report = f"# {topic}\n\n{introduction}\n\n---\n\n{content}\n\n---\n\n{conclusion}"
    
    return {'final_report': final_report}


# ================================
# 8. ë©”ì¸ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ êµ¬ì„±
# ================================

# Send import ì¶”ê°€
from langgraph.constants import Send


def build_research_graph():
    """ë©”ì¸ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê³  ë°˜í™˜"""
    # ì¸í„°ë·° ê·¸ë˜í”„ ë¨¼ì € ë¹Œë“œ
    interview_graph = build_interview_graph()
    
    # ë©”ì¸ ê·¸ë˜í”„ ìƒì„±
    builder = StateGraph(ResearchGraphState)
    
    # ë…¸ë“œ ì¶”ê°€
    builder.add_node("setting_consultants", setting_consultants)
    builder.add_node("initiate_all_interviews", initiate_all_interviews_node)
    builder.add_node("conduct_interview", interview_graph)
    builder.add_node("write_report", write_report)
    builder.add_node("write_introduction", write_introduction)
    builder.add_node("write_conclusion", write_conclusion)
    builder.add_node("finalize_report", finalize_report)
    
    # ì—£ì§€ ì—°ê²°
    builder.add_edge(START, "setting_consultants")
    builder.add_edge("setting_consultants", "initiate_all_interviews")
    builder.add_conditional_edges(
        "initiate_all_interviews",
        route_to_interviews,
        ["conduct_interview"]
    )
    builder.add_edge("conduct_interview", "write_report")
    builder.add_edge("conduct_interview", "write_introduction")
    builder.add_edge("conduct_interview", "write_conclusion")
    builder.add_edge(
        ["write_conclusion", "write_report", "write_introduction"], "finalize_report"
    )
    builder.add_edge("finalize_report", END)
    
    # ì»´íŒŒì¼
    memory = MemorySaver()
    return builder.compile(checkpointer=memory)


# ================================
# 9. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ================================

def invoke_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable = None,
):
    """
    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’
        config: ì‹¤í–‰ ì„¤ì •
        node_names: ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡
        callback: ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
    """
    def format_namespace(namespace):
        return namespace[-1].split(":")[0] if len(namespace) > 0 else "root graph"
    
    for namespace, chunk in graph.stream(
        inputs, config, stream_mode="updates", subgraphs=True
    ):
        for node_name, node_chunk in chunk.items():
            if len(node_names) > 0 and node_name not in node_names:
                continue
            
            if callback is not None:
                callback({"node": node_name, "content": node_chunk})
            else:
                print("\n" + "=" * 50)
                formatted_namespace = format_namespace(namespace)
                if formatted_namespace == "root graph":
                    print(f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m ğŸ”„")
                else:
                    print(
                        f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m in [\033[1;33m{formatted_namespace}\033[0m] ğŸ”„"
                    )
                print("- " * 25)
                
                if isinstance(node_chunk, dict):
                    for k, v in node_chunk.items():
                        if isinstance(v, BaseMessage):
                            v.pretty_print()
                        elif isinstance(v, list):
                            for list_item in v:
                                if isinstance(list_item, BaseMessage):
                                    list_item.pretty_print()
                                else:
                                    print(list_item)
                        else:
                            print(f"\033[1;32m{k}\033[0m:\n{v}")
                print("=" * 50)


def save_report_to_db(report: str, report_type: str = "daily"):
    """ë³´ê³ ì„œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    DB_URI = "postgresql+psycopg2://postgres:1234@localhost:5433/kalantir"
    
    db = SQLDatabase.from_uri(
        DB_URI,
        engine_args={
            "connect_args": {
                "options": "-c client_encoding=UTF8 -c search_path=ai"
            }
        }
    )
    
    query = text("""
    INSERT INTO reports (contents, report_type)
    VALUES (:final_report, :report_type)
    RETURNING id;
    """)
    
    params = {"final_report": report, "report_type": report_type}
    
    with db._engine.connect() as conn:
        result = conn.execute(query, params)
        new_id = result.scalar()
        conn.commit()
        
    return new_id


# ================================
# 10. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ë¹Œë“œ
    graph = build_research_graph()
    
    # ì‹¤í–‰ ì„¤ì •
    config = {"configurable": {"thread_id": "1"}}
    
    # ì´ˆê¸° ì…ë ¥
    initial_inputs = {
        "topic": "AI Integration: How can Samsung Electronics integrate AI to improve productivity and enhance its long-term intrinsic value?",
        "max_consultants": 3,
        "consultants": [],
        "sections": [],
        "introduction": "",
        "content": "",
        "conclusion": "",
        "final_report": ""
    }
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    print("=" * 80)
    print("ğŸš€ Starting Research Process...")
    print("=" * 80)
    
    invoke_graph(graph, initial_inputs, config)
    
    # ìµœì¢… ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°
    final_state = graph.get_state(config)
    final_report = final_state.values.get("final_report")
    
    # ë³´ê³ ì„œ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“„ FINAL REPORT")
    print("=" * 80)
    print(final_report)
    
    # ë³´ê³ ì„œ ì €ì¥ (ì˜µì…˜)
    report_id = save_report_to_db(final_report, "daily")
    print(f"\nâœ… Report saved to database with ID: {report_id}")
    
    return final_report


if __name__ == "__main__":
    main()