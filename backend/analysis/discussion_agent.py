"""
Discussion Agent - LangGraph ê¸°ë°˜ ì»¨ì„¤í„´íŠ¸ ì¸í„°ë·° ë° ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ ì „ë¬¸ê°€(ì»¨ì„¤í„´íŠ¸) í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•˜ì—¬ ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ ì¸í„°ë·°ë¥¼ ìˆ˜í–‰í•˜ê³ ,
ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""
import operator
from typing import Annotated, List, Callable

from util.consultants import Consultant, CONSULTANT_PROFILES
from util.path import PROMPT_DIR

from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage, get_buffer_string
from langchain_core.runnables import RunnableConfig
from langchain_core.prompts import load_prompt
from langchain_tavily import TavilySearch
from langchain_community.vectorstores import Qdrant
from langchain_community.utilities import SQLDatabase

from langgraph.constants import Send
from langgraph.graph import START, END, StateGraph, MessagesState
from langgraph.graph.state import CompiledStateGraph
from langgraph.checkpoint.memory import MemorySaver

from qdrant_client import QdrantClient
from sqlalchemy import text

load_dotenv()

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model='gpt-4o', temperature=0)

class SearchQuery(BaseModel):
    """ê²€ìƒ‰ ì¿¼ë¦¬ ëª¨ë¸"""
    search_query: str = Field(None, description="Search query for retrieval.")

def debug_reducer(old, new):
    """State update ë™ê¸°í™” error í•´ê²°ìš© reducer"""
    return new

class InterviewState(MessagesState):
    """ì¸í„°ë·° ìƒíƒœ"""
    max_num_turns: int
    context: Annotated[list, operator.add]
    consultant: Consultant
    interview: str
    sections: list
    topic: Annotated[str, debug_reducer]

class ResearchGraphState(MessagesState):
    """ì „ì²´ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ìƒíƒœ"""
    topic: Annotated[str, debug_reducer]
    max_consultants: int
    human_consultant_feedback: str
    consultants: List[Consultant]
    sections: Annotated[list, operator.add]
    introduction: str
    content: str
    conclusion: str
    final_report: str

# ================================
# ì¸í„°ë·° ë…¸ë“œ í•¨ìˆ˜ë“¤
# ================================
def ask_question(state: InterviewState):
    """ì»¨ì„¤í„´íŠ¸ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    topic = state['topic']
    consultant = state['consultant']
    messages = state['messages']
    
    question_instructions = load_prompt(PROMPT_DIR / "question_instructions.yaml", encoding="utf-8")
    
    system_message = question_instructions.format(topic=topic, goals=consultant.persona)
    question = llm.invoke([SystemMessage(content=system_message)] + messages)
    
    return {'messages': [question]}

def search_web(state: InterviewState):
    
    consultant= state['consultant']
    topic=state['topic']
    
    """ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë…¸ë“œ"""
    search_instructions = load_prompt(PROMPT_DIR / "search_instructions.yaml", encoding="utf-8").format(
        name=consultant.name, role=consultant.role, 
        department=consultant.department, description=consultant.description, 
        topic=topic, domain_keywords=consultant.domain_keywords, 
        search_focus=consultant.search_focus)
    
    # ì‘ë‹µ í˜•ì‹ ì§€ì •
    structured_llm = llm.with_structured_output(SearchQuery)
    # LLMìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
    search_query = structured_llm.invoke([SystemMessage(search_instructions)] + state['messages'])
    
    tavily_search = TavilySearch(max_results=6)
    
    print('ê²€ìƒ‰ì¿¼ë¦¬')
    print(search_query.search_query)
    search_docs = tavily_search.invoke(search_query.search_query)
    print('ê²€ìƒ‰ê²°ê³¼')
    print(search_docs)
    
    return {"context": [search_docs]}

def create_retriever(collection_name: str, k: int = 20):
    """Qdrantì—ì„œ retriever ìƒì„±"""
    QDRANT_URL = 'http://localhost:6333'
    
    client = QdrantClient(url=QDRANT_URL)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    vectorstore = Qdrant(
        client=client,
        collection_name=collection_name,
        embeddings=embeddings
    )
    
    return vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": k}
    )

def search_rag(state: InterviewState):
    """RAGë¥¼ í†µí•´ ë‚´ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ"""
    COLLECTIONS = ['samsung_internal_db', 'samsung_external_web', 'samsung_external_pdf']
    retrievers = {name: create_retriever(name) for name in COLLECTIONS}
    
    query = state['topic']
    
    for name, retriever in retrievers.items():
        results = retriever.invoke(query)
        print(f"\n{name} results:")
        for doc in results[:2]:
            print("-", doc.metadata.get("source"), ":", doc.page_content[:100])
            
    return {"context": [results]}

def answer_question(state: InterviewState):
    """ì „ë¬¸ê°€ê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    consultant = state['consultant']
    messages = state['messages']
    context = state.get('context', [])
    
    answer_instructions = load_prompt(PROMPT_DIR / "answer_instructions.yaml", encoding="utf-8")
    
    system_message = answer_instructions.format(
        goals=consultant.persona,
        context=context
    )
    answer = llm.invoke([SystemMessage(content=system_message)] + messages)
    
    return {'messages': [answer]}

def save_interview(state: InterviewState):
    """ì¸í„°ë·° ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” ë…¸ë“œ"""
    messages = state['messages']
    interview = get_buffer_string(messages)
    
    return {'interview': interview}

def write_section(state: InterviewState):
    """ì¸í„°ë·° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¹ì…˜ì„ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    interview = state['interview']
    context = state['context']
    consultant = state['consultant']
    
    # TODO interview ë‚´ìš© í™•ì¸í•˜ì—¬ í™œìš© ê²°ì •
    print('***********ì»¨í…ìŠ¤íŠ¸ì‹œì‘************')
    print(context)
    print('***********ì»¨í…ìŠ¤íŠ¸ ë************')
    print('**********ì¸í„°ë·° ì‹œì‘*************')
    print(interview)
    print('**********ì¸í„°ë·° ë*************')
    
    section_writer_instructions = load_prompt(PROMPT_DIR / "section_writer_instructions.yaml", encoding="utf-8")
    
    system_message = section_writer_instructions.format(focus=consultant.persona)
    section = llm.invoke([
        SystemMessage(content=system_message),
        HumanMessage(content=f"Use this source material to create a section: {context}")
    ])
    
    return {'sections': [section.content]}

def route_messages(state: InterviewState):
    """ëŒ€í™” ì¢…ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜"""
    messages = state['messages']
    max_num_turns = state.get('max_num_turns', 3)
    num_responses = len([m for m in messages if isinstance(m, AIMessage)])
    
    if num_responses >= max_num_turns:
        return 'save_interview'
    
    last_question = messages[-2]
    
    if "Thank you so much for your help" in last_question.content:
        return 'save_interview'
    
    return 'ask_question'

# ================================
# ì¸í„°ë·° ê·¸ë˜í”„ êµ¬ì„±
# ================================
def build_interview_graph():
    """ì¸í„°ë·° ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê³  ë°˜í™˜"""
    interview_builder = StateGraph(InterviewState)
    
    # ë…¸ë“œ ì¶”ê°€
    interview_builder.add_node('ask_question', ask_question)
    interview_builder.add_node('search_web', search_web)
    interview_builder.add_node('search_rag', search_rag)
    interview_builder.add_node('answer_question', answer_question)
    interview_builder.add_node('save_interview', save_interview)
    interview_builder.add_node('write_section', write_section)
    
    # ì—£ì§€ ì—°ê²°
    interview_builder.add_edge(START, 'ask_question')
    interview_builder.add_edge('ask_question', 'search_web')
    interview_builder.add_edge('ask_question', 'search_rag')
    interview_builder.add_edge('search_web', 'answer_question')
    interview_builder.add_edge('search_rag', 'answer_question')
    interview_builder.add_conditional_edges(
        'answer_question', route_messages, ['ask_question', 'save_interview']
    )
    interview_builder.add_edge('save_interview', 'write_section')
    interview_builder.add_edge('write_section', END)
    
    memory = MemorySaver()
    return interview_builder.compile(checkpointer=memory).with_config(
        run_name='Conduct Interviews'
    )


# ================================
# ë©”ì¸ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ë“¤
# ================================

def setting_consultants(state: ResearchGraphState):
    # ìµœëŒ€ ì»¨ì„¤í„´íŠ¸ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    max_consultants = state.get("max_consultants", 3)
    
    # í”„ë¡œí•„ì„ Consultant ê°ì²´ë¡œ ë³€í™˜
    consultants = [
        Consultant(**profile) 
        for profile in CONSULTANT_PROFILES[:max_consultants]
    ]
    return {'consultants': consultants}

# TODO ë³‘ë ¬ì‹¤í–‰ì„ ìœ„í•´ ì¶”ê°€í•œ ë”ë¯¸ í•¨ìˆ˜ - ë¶ˆí•„ìš”ì‹œ ì œê±°
# ë…¸ë“œ í•¨ìˆ˜ (ìƒíƒœ ì—…ë°ì´íŠ¸ë§Œ)
def initiate_all_interviews_node(state: ResearchGraphState):
    # ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜ (pass-through) 
    return {}

def route_to_interviews(state: ResearchGraphState):
    """ì¸í„°ë·° ë¼ìš°íŒ… í•¨ìˆ˜"""
    max_consultants = state.get('max_consultants', 3)
    
    return [
        Send("conduct_interview", {
            "consultant": consultant,
            "messages": [
                HumanMessage(content="Welcome! We'll begin the interview now.")
            ],
            "max_num_turns": 3,
            "context": [],
            "topic": state['topic'],
            "sections": state['sections']
        })
        for consultant in state['consultants'][:max_consultants]
    ]

def write_report(state: ResearchGraphState):
    """ë©”ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    sections = state['sections']
    topic = state['topic']
    
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])
    
    report_writer_instructions = load_prompt(PROMPT_DIR / "report_writer_instructions.yaml", encoding="utf-8")
    
    system_message = report_writer_instructions.format(
        topic=topic,
        context=formatted_str_sections
    )
    report = llm.invoke([SystemMessage(content=system_message)])
    
    return {'content': report.content}

def write_introduction(state: ResearchGraphState):
    """ì¸íŠ¸ë¡œë¥¼ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    topic = state['topic']
    sections = state['sections']
    
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])
    
    intro_conclusion_instructions = load_prompt(PROMPT_DIR / "intro_conclusion_instructions.yaml", encoding="utf-8")
    
    instructions = intro_conclusion_instructions.format(
        topic=topic,
        formatted_str_sections=formatted_str_sections
    )
    intro = llm.invoke([
        SystemMessage(content=instructions),
        HumanMessage(content=f"Here is the main body:\n\n{formatted_str_sections}")
    ])
    
    return {'introduction': intro.content}

def write_conclusion(state: ResearchGraphState):
    """ê²°ë¡ ì„ ì‘ì„±í•˜ëŠ” ë…¸ë“œ"""
    topic = state['topic']
    sections = state['sections']
    
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])
    
    intro_conclusion_instructions = load_prompt(PROMPT_DIR / "intro_conclusion_instructions.yaml", encoding="utf-8")
    
    instructions = intro_conclusion_instructions.format(
        topic=topic,
        formatted_str_sections=formatted_str_sections
    )
    conclusion = llm.invoke([
        SystemMessage(content=instructions),
        HumanMessage(content=f"Here is the main body:\n\n{formatted_str_sections}")
    ])
    
    return {'conclusion': conclusion.content}

def finalize_report(state: ResearchGraphState):
    """ìµœì¢… ë³´ê³ ì„œë¥¼ ì™„ì„±í•˜ëŠ” ë…¸ë“œ"""
    introduction = state['introduction']
    content = state['content']
    conclusion = state['conclusion']
    topic = state['topic']
    
    final_report = f"# {topic}\n\n{introduction}\n\n---\n\n{content}\n\n---\n\n{conclusion}"
    
    return {'final_report': final_report}


# ================================
# ë©”ì¸ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ êµ¬ì„±
# ================================
def build_research_graph():
    """ë©”ì¸ ë¦¬ì„œì¹˜ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê³  ë°˜í™˜"""
    # ì¸í„°ë·° ê·¸ë˜í”„ ë¨¼ì € ë¹Œë“œ
    interview_graph = build_interview_graph()
    
    # ë©”ì¸ ê·¸ë˜í”„ ìƒì„±
    builder = StateGraph(ResearchGraphState)
    
    # ë…¸ë“œ ì¶”ê°€
    builder.add_node("setting_consultants", setting_consultants)
    builder.add_node("initiate_all_interviews", initiate_all_interviews_node)
    builder.add_node("conduct_interview", interview_graph)
    builder.add_node("write_report", write_report)
    builder.add_node("write_introduction", write_introduction)
    builder.add_node("write_conclusion", write_conclusion)
    builder.add_node("finalize_report", finalize_report)
    
    # ì—£ì§€ ì—°ê²°
    builder.add_edge(START, "setting_consultants")
    builder.add_edge("setting_consultants", "initiate_all_interviews")
    builder.add_conditional_edges(
        "initiate_all_interviews",
        route_to_interviews,
        ["conduct_interview"]
    )
    builder.add_edge("conduct_interview", "write_report")
    builder.add_edge("conduct_interview", "write_introduction")
    builder.add_edge("conduct_interview", "write_conclusion")
    builder.add_edge(
        ["write_conclusion", "write_report", "write_introduction"], "finalize_report"
    )
    builder.add_edge("finalize_report", END)
    
    # ì»´íŒŒì¼
    memory = MemorySaver()
    return builder.compile(checkpointer=memory)


# ================================
# 9. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ================================

def invoke_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable = None,
):
    """
    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’
        config: ì‹¤í–‰ ì„¤ì •
        node_names: ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡
        callback: ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
    """
    def format_namespace(namespace):
        return namespace[-1].split(":")[0] if len(namespace) > 0 else "root graph"
    
    for namespace, chunk in graph.stream(
        inputs, config, stream_mode="updates", subgraphs=True
    ):
        for node_name, node_chunk in chunk.items():
            if len(node_names) > 0 and node_name not in node_names:
                continue
            
            if callback is not None:
                callback({"node": node_name, "content": node_chunk})
            else:
                print("\n" + "=" * 50)
                formatted_namespace = format_namespace(namespace)
                if formatted_namespace == "root graph":
                    print(f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m ğŸ”„")
                else:
                    print(
                        f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m in [\033[1;33m{formatted_namespace}\033[0m] ğŸ”„"
                    )
                print("- " * 25)
                
                if isinstance(node_chunk, dict):
                    for k, v in node_chunk.items():
                        if isinstance(v, BaseMessage):
                            v.pretty_print()
                        elif isinstance(v, list):
                            for list_item in v:
                                if isinstance(list_item, BaseMessage):
                                    list_item.pretty_print()
                                else:
                                    print(list_item)
                        else:
                            print(f"\033[1;32m{k}\033[0m:\n{v}")
                print("=" * 50)


def save_report_to_db(report: str, report_type: str = "daily"):
    """ë³´ê³ ì„œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    DB_URI = "postgresql+psycopg2://postgres:1234@localhost:5433/kalantir"
    
    db = SQLDatabase.from_uri(
        DB_URI,
        engine_args={
            "connect_args": {
                "options": "-c client_encoding=UTF8 -c search_path=ai"
            }
        }
    )
    
    query = text("""
    INSERT INTO reports (contents, report_type)
    VALUES (:final_report, :report_type)
    RETURNING id;
    """)
    
    params = {"final_report": report, "report_type": report_type}
    
    with db._engine.connect() as conn:
        result = conn.execute(query, params)
        new_id = result.scalar()
        conn.commit()
        
    return new_id


# ================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¦¬ì„œì¹˜ ê·¸ë˜í”„ ë¹Œë“œ
    graph = build_research_graph()
    
    # ì‹¤í–‰ ì„¤ì •
    config = {"configurable": {"thread_id": "1"}}
    
    # ì´ˆê¸° ì…ë ¥
    initial_inputs = {
        "topic": "AI Integration: How can Samsung Electronics integrate AI to improve productivity and enhance its long-term intrinsic value?",
        "max_consultants": 3,
        "consultants": [],
        "sections": [],
        "introduction": "",
        "content": "",
        "conclusion": "",
        "final_report": ""
    }
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    print("=" * 80)
    print("Starting Research Process...")
    print("=" * 80)
    
    invoke_graph(graph, initial_inputs, config)
    
    # ìµœì¢… ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°
    final_state = graph.get_state(config)
    final_report = final_state.values.get("final_report")
    
    # ë³´ê³ ì„œ ì¶œë ¥
    print("\n" + "=" * 80)
    print("FINAL REPORT")
    print("=" * 80)
    print(final_report)
    
    # ë³´ê³ ì„œ ì €ì¥ (ì˜µì…˜)
    report_id = save_report_to_db(final_report, "daily")
    print(f"\nReport saved to database with ID: {report_id}")
    
    return final_report


if __name__ == "__main__":
    main()